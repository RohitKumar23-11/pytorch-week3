{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRd8eLvf7_qQ",
        "outputId": "586b4783-8866-4a85-b210-ad6d2e64c5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=3.0004 train_acc=0.2203 val_loss=2.3169 val_acc=0.2845 BLEU=2.19\n",
            "Epoch 2: train_loss=2.0995 train_acc=0.3219 val_loss=1.8911 val_acc=0.3397 BLEU=4.69\n",
            "Epoch 3: train_loss=1.7584 train_acc=0.3924 val_loss=1.6814 val_acc=0.4017 BLEU=6.59\n",
            "Epoch 4: train_loss=1.5394 train_acc=0.4638 val_loss=1.5480 val_acc=0.4472 BLEU=7.98\n",
            "Epoch 5: train_loss=1.3809 train_acc=0.5172 val_loss=1.4125 val_acc=0.5127 BLEU=10.81\n",
            "Epoch 6: train_loss=1.2371 train_acc=0.5725 val_loss=1.2573 val_acc=0.5599 BLEU=13.17\n",
            "Epoch 7: train_loss=1.1087 train_acc=0.6143 val_loss=1.1984 val_acc=0.5847 BLEU=13.27\n",
            "Epoch 8: train_loss=1.0094 train_acc=0.6456 val_loss=1.1178 val_acc=0.6191 BLEU=15.51\n",
            "Epoch 9: train_loss=0.9391 train_acc=0.6757 val_loss=1.0574 val_acc=0.6372 BLEU=16.69\n",
            "Epoch 10: train_loss=0.8593 train_acc=0.6992 val_loss=0.9490 val_acc=0.6803 BLEU=19.41\n",
            "Epoch 11: train_loss=0.7746 train_acc=0.7300 val_loss=0.9203 val_acc=0.6898 BLEU=20.93\n",
            "Epoch 12: train_loss=0.7361 train_acc=0.7411 val_loss=0.9155 val_acc=0.6897 BLEU=20.59\n",
            "\n",
            "Done. Artifacts saved to runs/mt/ (png, md, best_model.pt).\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "toy_transformer_mt.py\n",
        "\n",
        "Minimal Transformer encoder-decoder implemented from scratch (no nn.Transformer).\n",
        "- Uses a toy copy/translation task (source -> target is source reversed and shifted with start/stop tokens).\n",
        "- Implements Embeddings, Sinusoidal positional encoding, Multi-head attention, Feed-forward, LayerNorm, masking.\n",
        "- Training loop with plotting and attention visualizations saved to runs/mt/ as requested.\n",
        "\n",
        "Outputs (saved in runs/mt/):\n",
        "- curves_mt.png\n",
        "- attention_layer{L}_head{H}.png\n",
        "- masks_demo.png\n",
        "- decodes_table.png\n",
        "- bleu_report.png\n",
        "- report_one_page.md\n",
        "\n",
        "Run: python toy_transformer_mt.py\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "from typing import Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# -----------------------------\n",
        "# Toy dataset (integer sequences)\n",
        "# -----------------------------\n",
        "# We'll build a toy parallel corpus where input sequences are random integer sequences\n",
        "# and target is the reversed input with a shift (target_token = input_token + 2 mod (vocab-4)),\n",
        "# plus BOS and EOS tokens. This gives the model a non-trivial mapping to learn.\n",
        "\n",
        "class ToyTranslationDataset(Dataset):\n",
        "    def __init__(self, n_samples=10000, min_len=3, max_len=10, vocab_size=50):\n",
        "        self.n_samples = n_samples\n",
        "        self.min_len = min_len\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # special tokens\n",
        "        self.PAD = 0\n",
        "        self.BOS = 1\n",
        "        self.EOS = 2\n",
        "        self.OOV = 3\n",
        "        self.offset = 4  # so tokens start from 4\n",
        "\n",
        "        self.samples = [self._make_sample() for _ in range(n_samples)]\n",
        "\n",
        "    def _make_sample(self):\n",
        "        L = random.randint(self.min_len, self.max_len)\n",
        "        seq = [random.randint(self.offset, self.vocab_size - 1) for _ in range(L)]\n",
        "        # target mapping: reverse and shift by +1 within token space (mod reserved area)\n",
        "        tgt = [((t - self.offset + 1) % (self.vocab_size - self.offset)) + self.offset for t in reversed(seq)]\n",
        "        return seq, tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.samples[idx]\n",
        "        return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
        "\n",
        "# collate function\n",
        "\n",
        "def collate_batch(batch, pad_idx=0, bos_idx=1, eos_idx=2):\n",
        "    src_seqs, tgt_seqs = zip(*batch)\n",
        "    src_lens = [len(s) for s in src_seqs]\n",
        "    tgt_lens = [len(t) for t in tgt_seqs]\n",
        "    max_src = max(src_lens)\n",
        "    max_tgt = max(tgt_lens) + 2  # BOS and EOS\n",
        "\n",
        "    batch_size = len(batch)\n",
        "    src_pad = torch.full((batch_size, max_src), pad_idx, dtype=torch.long)\n",
        "    tgt_input = torch.full((batch_size, max_tgt), pad_idx, dtype=torch.long)\n",
        "    tgt_output = torch.full((batch_size, max_tgt), pad_idx, dtype=torch.long)\n",
        "\n",
        "    for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n",
        "        src_pad[i, : len(s)] = s\n",
        "        tgt_input[i, 0] = bos_idx\n",
        "        tgt_input[i, 1 : 1 + len(t)] = t\n",
        "        tgt_input[i, 1 + len(t)] = eos_idx\n",
        "\n",
        "        tgt_output[i, : len(t)] = t\n",
        "        tgt_output[i, len(t)] = eos_idx\n",
        "        # note: tgt_output has no initial BOS; shifted relative to tgt_input\n",
        "\n",
        "    return src_pad, tgt_input, tgt_output\n",
        "\n",
        "# -----------------------------\n",
        "# Positional encoding\n",
        "# -----------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, learned=False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        if learned:\n",
        "            self.pe = nn.Parameter(torch.randn(max_len, d_model))\n",
        "        else:\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            self.register_buffer('pe', pe)  # (max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:seq_len, :].unsqueeze(0)\n",
        "\n",
        "# -----------------------------\n",
        "# Scaled Dot-Product Attention + MultiHead\n",
        "# -----------------------------\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
        "    # q, k, v: (batch, n_heads, seq_len, head_dim)\n",
        "    dk = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dk)  # (batch, n_heads, seq_q, seq_k)\n",
        "    if mask is not None:\n",
        "        # mask should be broadcastable to scores; use -1e9 for masked positions\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attn = torch.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        attn = dropout(attn)\n",
        "    out = torch.matmul(attn, v)\n",
        "    return out, attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query/key/value: (batch, seq_len, d_model)\n",
        "        B = query.size(0)\n",
        "        Q = self.q_linear(query).view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_linear(key).view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_linear(value).view(B, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # mask: (batch, 1, seq_q, seq_k) or (batch, n_heads, seq_q, seq_k)\n",
        "        out, attn = scaled_dot_product_attention(Q, K, V, mask=mask, dropout=self.dropout)\n",
        "        # out: (batch, n_heads, seq_len, head_dim)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, -1, self.d_model)\n",
        "        out = self.out_linear(out)\n",
        "        return out, attn\n",
        "\n",
        "# -----------------------------\n",
        "# FeedForward\n",
        "# -----------------------------\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -----------------------------\n",
        "# Encoder / Decoder Layer\n",
        "# -----------------------------\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        # Self-attention\n",
        "        attn_out, attn = self.self_attn(x, x, x, mask=src_mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + self.dropout(ff_out)\n",
        "        x = self.norm2(x)\n",
        "        return x, attn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        self_attn_out, self_attn = self.self_attn(x, x, x, mask=tgt_mask)\n",
        "        x = x + self.dropout(self_attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        cross_attn_out, cross_attn = self.cross_attn(x, enc_out, enc_out, mask=memory_mask)\n",
        "        x = x + self.dropout(cross_attn_out)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        ff_out = self.ff(x)\n",
        "        x = x + self.dropout(ff_out)\n",
        "        x = self.norm3(x)\n",
        "        return x, self_attn, cross_attn\n",
        "\n",
        "# -----------------------------\n",
        "# Encoder, Decoder, Transformer\n",
        "# -----------------------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, max_len=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        # src: (batch, src_len)\n",
        "        x = self.tok_emb(src) * math.sqrt(self.tok_emb.embedding_dim)\n",
        "        x = self.pos_enc(x)\n",
        "\n",
        "        attentions = []\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x, src_mask)\n",
        "            attentions.append(attn)\n",
        "        x = self.norm(x)\n",
        "        return x, attentions\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, max_len=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.tok_emb(tgt) * math.sqrt(self.tok_emb.embedding_dim)\n",
        "        x = self.pos_enc(x)\n",
        "\n",
        "        self_attns = []\n",
        "        cross_attns = []\n",
        "        for layer in self.layers:\n",
        "            x, self_attn, cross_attn = layer(x, enc_out, tgt_mask, memory_mask)\n",
        "            self_attns.append(self_attn)\n",
        "            cross_attns.append(cross_attn)\n",
        "        x = self.norm(x)\n",
        "        return x, self_attns, cross_attns\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=128, n_layers=2, n_heads=4, d_ff=256, max_len=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, n_layers, n_heads, d_ff, max_len, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab, d_model, n_layers, n_heads, d_ff, max_len, dropout)\n",
        "        self.out = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
        "        enc_out, enc_attns = self.encoder(src, src_mask)\n",
        "        dec_out, self_attns, cross_attns = self.decoder(tgt, enc_out, tgt_mask, memory_mask)\n",
        "        logits = self.out(dec_out)\n",
        "        return logits, enc_attns, self_attns, cross_attns\n",
        "\n",
        "# -----------------------------\n",
        "# Mask helpers\n",
        "# -----------------------------\n",
        "\n",
        "def make_src_mask(src, pad_idx=0):\n",
        "    # src: (batch, src_len)\n",
        "    # return mask shaped (batch, 1, 1, src_len) or broadcastable: 1 where allowed\n",
        "    mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return mask  # True=allowed\n",
        "\n",
        "\n",
        "def make_tgt_mask(tgt, pad_idx=0):\n",
        "    # tgt: (batch, tgt_len)\n",
        "    batch, tgt_len = tgt.size()\n",
        "    pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch,1,1,tgt_len)\n",
        "    # subsequent mask (causal)\n",
        "    subsequent = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
        "    subsequent = subsequent.unsqueeze(0).unsqueeze(1)  # (1,1,tgt_len,tgt_len)\n",
        "    mask = pad_mask & subsequent\n",
        "    return mask\n",
        "\n",
        "# For memory mask: cross-attention should not attend to PAD in source\n",
        "\n",
        "def make_memory_mask(src, tgt, pad_idx=0):\n",
        "    # returns (batch, 1, tgt_len, src_len)\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch,1,1,src_len)\n",
        "    tgt_len = tgt.size(1)\n",
        "    return src_mask.repeat(1, 1, tgt_len, 1)\n",
        "\n",
        "# -----------------------------\n",
        "# Training utilities\n",
        "# -----------------------------\n",
        "\n",
        "def compute_loss_and_accuracy(logits, targets, pad_idx=0):\n",
        "    # logits: (batch, tgt_len, vocab)\n",
        "    batch, tgt_len, vocab = logits.size()\n",
        "    logits_flat = logits.view(-1, vocab)\n",
        "    targets_flat = targets.view(-1)\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=pad_idx)\n",
        "    # accuracy (ignoring pad)\n",
        "    preds = logits_flat.argmax(dim=-1)\n",
        "    mask = (targets_flat != pad_idx)\n",
        "    if mask.sum() == 0:\n",
        "        acc = torch.tensor(0.0)\n",
        "    else:\n",
        "        acc = (preds[mask] == targets_flat[mask]).float().mean()\n",
        "    return loss, acc.item()\n",
        "\n",
        "# Simple corpus BLEU (unigram..4-gram precision + brevity penalty)\n",
        "\n",
        "def simple_corpus_bleu(references: List[List[int]], hypotheses: List[List[int]], max_n=4):\n",
        "    # all tokens are integers; compute modified precision per n\n",
        "    import collections\n",
        "\n",
        "    precisions = []\n",
        "    for n in range(1, max_n + 1):\n",
        "        num = 0\n",
        "        den = 0\n",
        "        for ref, hyp in zip(references, hypotheses):\n",
        "            ref_ngrams = collections.Counter([tuple(ref[i:i+n]) for i in range(len(ref)-n+1)]) if len(ref) >= n else collections.Counter()\n",
        "            hyp_ngrams = collections.Counter([tuple(hyp[i:i+n]) for i in range(len(hyp)-n+1)]) if len(hyp) >= n else collections.Counter()\n",
        "            overlap = 0\n",
        "            total = sum(hyp_ngrams.values())\n",
        "            for ng in hyp_ngrams:\n",
        "                overlap += min(hyp_ngrams[ng], ref_ngrams.get(ng, 0))\n",
        "            num += overlap\n",
        "            den += total\n",
        "        if den == 0:\n",
        "            precisions.append(0.0)\n",
        "        else:\n",
        "            precisions.append(num / den)\n",
        "    # geometric mean; if any precision is zero -> BLEU zero unless smoothing\n",
        "    smooth = 1e-9\n",
        "    gm = math.exp(sum((1.0/max_n) * math.log(p + smooth) for p in precisions))\n",
        "    # brevity penalty\n",
        "    ref_len = sum(len(r) for r in references)\n",
        "    hyp_len = sum(len(h) for h in hypotheses)\n",
        "    bp = 1.0\n",
        "    if hyp_len == 0:\n",
        "        bp = 0.0\n",
        "    elif hyp_len < ref_len:\n",
        "        bp = math.exp(1 - ref_len / hyp_len)\n",
        "    bleu = bp * gm\n",
        "    return bleu * 100\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval\n",
        "# -----------------------------\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, pad_idx=0):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    steps = 0\n",
        "    for src, tgt_in, tgt_out in dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt_in = tgt_in.to(device)\n",
        "        tgt_out = tgt_out.to(device)\n",
        "        src_mask = make_src_mask(src, pad_idx=pad_idx)\n",
        "        tgt_mask = make_tgt_mask(tgt_in, pad_idx=pad_idx)\n",
        "        memory_mask = make_memory_mask(src, tgt_in, pad_idx=pad_idx)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, enc_attn, self_attns, cross_attns = model(src, tgt_in, src_mask, tgt_mask, memory_mask)\n",
        "        loss, acc = compute_loss_and_accuracy(logits, tgt_out, pad_idx=pad_idx)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "        steps += 1\n",
        "    return total_loss / steps, total_acc / steps\n",
        "\n",
        "\n",
        "def eval_epoch(model, dataloader, pad_idx=0):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    steps = 0\n",
        "    all_refs = []\n",
        "    all_hyps = []\n",
        "    attention_samples = None\n",
        "    masks_sample = None\n",
        "    with torch.no_grad():\n",
        "        for i, (src, tgt_in, tgt_out) in enumerate(dataloader):\n",
        "            src = src.to(device)\n",
        "            tgt_in = tgt_in.to(device)\n",
        "            tgt_out = tgt_out.to(device)\n",
        "            src_mask = make_src_mask(src, pad_idx=pad_idx)\n",
        "            tgt_mask = make_tgt_mask(tgt_in, pad_idx=pad_idx)\n",
        "            memory_mask = make_memory_mask(src, tgt_in, pad_idx=pad_idx)\n",
        "\n",
        "            logits, enc_attn, self_attns, cross_attns = model(src, tgt_in, src_mask, tgt_mask, memory_mask)\n",
        "            loss, acc = compute_loss_and_accuracy(logits, tgt_out, pad_idx=pad_idx)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "            steps += 1\n",
        "\n",
        "            # decode greedy for BLEU\n",
        "            preds = logits.argmax(dim=-1).cpu().tolist()\n",
        "            refs = tgt_out.cpu().tolist()\n",
        "            for r, p in zip(refs, preds):\n",
        "                # remove pad and eos if present\n",
        "                r = [token for token in r if token != 0]\n",
        "                p = [token for token in p if token != 0]\n",
        "                all_refs.append(r)\n",
        "                all_hyps.append(p)\n",
        "\n",
        "            # keep one batch of attentions and masks for visualization\n",
        "            if attention_samples is None:\n",
        "                attention_samples = (enc_attn, self_attns, cross_attns)\n",
        "                masks_sample = (src_mask.cpu(), tgt_mask.cpu(), memory_mask.cpu())\n",
        "\n",
        "    bleu = simple_corpus_bleu(all_refs, all_hyps)\n",
        "    return total_loss / steps, total_acc / steps, bleu, attention_samples, masks_sample\n",
        "\n",
        "# Greedy decode function (for inference)\n",
        "\n",
        "def greedy_decode(model, src, max_len=30, pad_idx=0, bos_idx=1, eos_idx=2):\n",
        "    model.eval()\n",
        "    src = src.to(device)\n",
        "    src_mask = make_src_mask(src, pad_idx)\n",
        "    with torch.no_grad():\n",
        "        enc_out, _ = model.encoder(src, src_mask)\n",
        "        batch_size = src.size(0)\n",
        "        ys = torch.full((batch_size, 1), bos_idx, dtype=torch.long, device=device)\n",
        "        for i in range(max_len):\n",
        "            tgt_mask = make_tgt_mask(ys, pad_idx=pad_idx)\n",
        "            memory_mask = make_memory_mask(src, ys, pad_idx=pad_idx)\n",
        "            dec_out, self_attns, cross_attns = model.decoder(ys, enc_out, tgt_mask, memory_mask)\n",
        "            logits = model.out(dec_out)  # (batch, seq, vocab)\n",
        "            next_token = logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n",
        "            ys = torch.cat([ys, next_token], dim=1)\n",
        "            if (next_token == eos_idx).all():\n",
        "                break\n",
        "    return ys[:, 1:]\n",
        "\n",
        "# -----------------------------\n",
        "# Visualization helpers\n",
        "# -----------------------------\n",
        "\n",
        "def ensure_dirs():\n",
        "    os.makedirs('runs/mt', exist_ok=True)\n",
        "\n",
        "\n",
        "def plot_curves(train_losses, val_losses, filename='runs/mt/curves_mt.png'):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(train_losses, label='train_loss')\n",
        "    plt.plot(val_losses, label='val_loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_attention_heatmaps(attention_samples, filename_pattern='runs/mt/attention_layer{L}_head{H}.png'):\n",
        "    # attention_samples: (enc_attn, self_attns, cross_attns)\n",
        "    enc_attn, self_attns, cross_attns = attention_samples\n",
        "    # enc_attn: list of per-layer attention matrices shape (batch,n_heads,seq,seq)\n",
        "    # We'll take the first batch element and visualize each head for each layer\n",
        "    for layer_idx, layer_attn in enumerate(enc_attn):\n",
        "        arr = layer_attn[0].cpu().numpy()  # (n_heads, seq, seq)\n",
        "        n_heads = arr.shape[0]\n",
        "        for h in range(n_heads):\n",
        "            plt.figure(figsize=(4,4))\n",
        "            plt.imshow(arr[h], aspect='auto')\n",
        "            plt.colorbar()\n",
        "            plt.title(f'Encoder Layer {layer_idx} Head {h}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(filename_pattern.format(L=layer_idx, H=h))\n",
        "            plt.close()\n",
        "\n",
        "    # self and cross attns from decoder\n",
        "    for layer_idx, layer_attn in enumerate(self_attns):\n",
        "        arr = layer_attn[0].cpu().numpy()\n",
        "        n_heads = arr.shape[0]\n",
        "        for h in range(n_heads):\n",
        "            plt.figure(figsize=(4,4))\n",
        "            plt.imshow(arr[h], aspect='auto')\n",
        "            plt.colorbar()\n",
        "            plt.title(f'Decoder Self Layer {layer_idx} Head {h}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(filename_pattern.format(L=f'decself{layer_idx}', H=h))\n",
        "            plt.close()\n",
        "\n",
        "    for layer_idx, layer_attn in enumerate(cross_attns):\n",
        "        arr = layer_attn[0].cpu().numpy()\n",
        "        n_heads = arr.shape[0]\n",
        "        for h in range(n_heads):\n",
        "            plt.figure(figsize=(4,4))\n",
        "            plt.imshow(arr[h], aspect='auto')\n",
        "            plt.colorbar()\n",
        "            plt.title(f'Decoder Cross Layer {layer_idx} Head {h}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(filename_pattern.format(L=f'deccross{layer_idx}', H=h))\n",
        "            plt.close()\n",
        "\n",
        "\n",
        "def save_masks_demo(masks_sample, filename='runs/mt/masks_demo.png'):\n",
        "    src_mask, tgt_mask, mem_mask = masks_sample\n",
        "    src_m = src_mask[0,0,0].unsqueeze(0).numpy()\n",
        "    tgt_m = tgt_mask[0,0,0].unsqueeze(0).numpy()\n",
        "    mem_m = mem_mask[0,0].numpy()  # already 2D\n",
        "\n",
        "    fig, axes = plt.subplots(1,3, figsize=(9,3))\n",
        "    axes[0].imshow(src_m, aspect='auto')\n",
        "    axes[0].set_title('src_mask (1=token)')\n",
        "    axes[1].imshow(tgt_m, aspect='auto')\n",
        "    axes[1].set_title('tgt_mask (causal & pad)')\n",
        "    axes[2].imshow(mem_m, aspect='auto')\n",
        "    axes[2].set_title('memory_mask')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def save_decodes_table(dataset, model, filename='runs/mt/decodes_table.png', n=10):\n",
        "    # sample n examples from dataset, decode and display table\n",
        "    samples = [dataset[i] for i in range(n)]\n",
        "    srcs = [s for s,t in samples]\n",
        "    tgts = [t for s,t in samples]\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(srcs, batch_first=True, padding_value=0)\n",
        "    hyps = greedy_decode(model, src_padded, max_len=30).cpu().tolist()\n",
        "\n",
        "    # convert to strings of ints\n",
        "    rows = []\n",
        "    for s, t, h in zip(srcs, tgts, hyps):\n",
        "        rows.append((\" \".join(map(str, s.tolist())), \" \".join(map(str, t.tolist())), \" \".join(map(str, [tok for tok in h if tok != 0]))))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, n*0.5 + 1))\n",
        "    ax.axis('off')\n",
        "    table = ax.table(cellText=rows, colLabels=['src', 'gold_tgt', 'decoded'], loc='center')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(8)\n",
        "    table.scale(1, 1.2)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_bleu_report(bleu, filename='runs/mt/bleu_report.png'):\n",
        "    fig, ax = plt.subplots(figsize=(4,2))\n",
        "    ax.axis('off')\n",
        "    ax.text(0.5, 0.5, f'Corpus BLEU: {bleu:.2f}', fontsize=16, ha='center', va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "def save_one_page_report(images, filename='runs/mt/report_one_page.md'):\n",
        "    # simple markdown report listing images with one-line captions\n",
        "    lines = ['# One-page Report\\n']\n",
        "    captions = {\n",
        "        'runs/mt/curves_mt.png': 'Loss curves (train & val).',\n",
        "        'runs/mt/masks_demo.png': 'Visualization of source/target/memory masks.',\n",
        "        'runs/mt/decodes_table.png': 'Comparison of decoded outputs with ground truth (10 samples).',\n",
        "        'runs/mt/bleu_report.png': 'Corpus BLEU score summary.',\n",
        "    }\n",
        "    for img in images:\n",
        "        cap = captions.get(img, '')\n",
        "        lines.append(f'![{os.path.basename(img)}]({img})\\n\\n*{cap}*\\n')\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "# -----------------------------\n",
        "# Main: construct dataset, model, train\n",
        "# -----------------------------\n",
        "\n",
        "def main():\n",
        "    ensure_dirs()\n",
        "    # dataset\n",
        "    VOCAB = 50\n",
        "    train_ds = ToyTranslationDataset(n_samples=3000, min_len=3, max_len=8, vocab_size=VOCAB)\n",
        "    val_ds = ToyTranslationDataset(n_samples=500, min_len=3, max_len=8, vocab_size=VOCAB)\n",
        "\n",
        "    BATCH = 64\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, collate_fn=lambda b: collate_batch(b, pad_idx=0, bos_idx=1, eos_idx=2))\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False, collate_fn=lambda b: collate_batch(b, pad_idx=0, bos_idx=1, eos_idx=2))\n",
        "\n",
        "    model = Transformer(src_vocab=VOCAB, tgt_vocab=VOCAB, d_model=128, n_layers=2, n_heads=4, d_ff=256, max_len=100).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    EPOCHS = 12\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_bleu = 0.0\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, pad_idx=0)\n",
        "        val_loss, val_acc, bleu, attention_samples, masks_sample = eval_epoch(model, val_loader, pad_idx=0)\n",
        "        train_losses.append(tr_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Epoch {epoch}: train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} BLEU={bleu:.2f}')\n",
        "\n",
        "        # save best\n",
        "        if bleu > best_bleu:\n",
        "            best_bleu = bleu\n",
        "            torch.save(model.state_dict(), 'runs/mt/best_model.pt')\n",
        "\n",
        "        # save visuals each epoch (lightweight)\n",
        "        plot_curves(train_losses, val_losses)\n",
        "        if attention_samples is not None:\n",
        "            save_attention_heatmaps(attention_samples)\n",
        "        if masks_sample is not None:\n",
        "            save_masks_demo(masks_sample)\n",
        "        save_bleu_report(bleu)\n",
        "\n",
        "    # final decodes table using train_ds samples\n",
        "    save_decodes_table(val_ds, model)\n",
        "\n",
        "    # final BLEU\n",
        "    _, _, bleu_final, _, _ = eval_epoch(model, val_loader, pad_idx=0)\n",
        "    save_bleu_report(bleu_final)\n",
        "\n",
        "    # one-page markdown report\n",
        "    images = ['runs/mt/curves_mt.png', 'runs/mt/masks_demo.png', 'runs/mt/decodes_table.png', 'runs/mt/bleu_report.png']\n",
        "    save_one_page_report(images)\n",
        "\n",
        "    print('\\nDone. Artifacts saved to runs/mt/ (png, md, best_model.pt).')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}